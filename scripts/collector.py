"""
å…¬å¼RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°ã®AI/MLãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
è‹±èªã‚½ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥å–å¾— â†’ LLMã§æ—¥æœ¬èªã«è¶…è¦ç´„
"""
import json
import os
import sys
import re
from datetime import datetime, timedelta
import time

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import feedparser
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: feedparserãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("pip install feedparser ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: google-genaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("pip install google-genai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


# ä¿¡é ¼æ€§ã®é«˜ã„å…¬å¼RSSãƒ•ã‚£ãƒ¼ãƒ‰
RSS_FEEDS = [
    {
        "name": "OpenAI Blog",
        "url": "https://openai.com/blog/rss.xml",
        "priority": 1,  # æœ€å„ªå…ˆ
    },
    {
        "name": "Google AI Blog", 
        "url": "https://blog.google/technology/ai/rss/",
        "priority": 1,
    },
    {
        "name": "Anthropic",
        "url": "https://www.anthropic.com/index.xml",
        "priority": 1,
    },
    {
        "name": "Microsoft Research",
        "url": "https://www.microsoft.com/en-us/research/feed/",
        "priority": 2,
    },
    {
        "name": "AWS Machine Learning",
        "url": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "priority": 2,
    },
    {
        "name": "Hugging Face Blog",
        "url": "https://huggingface.co/blog/feed.xml",
        "priority": 2,
    },
]


def fetch_rss_entries(max_age_days=7):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—"""
    all_entries = []
    cutoff_date = datetime.now() - timedelta(days=max_age_days)
    
    for feed_info in RSS_FEEDS:
        try:
            print(f"ğŸ“¡ {feed_info['name']} ã‚’å–å¾—ä¸­...")
            feed = feedparser.parse(feed_info["url"])
            
            if feed.bozo and not feed.entries:
                print(f"  âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—: {feed_info['name']}")
                continue
            
            for entry in feed.entries[:5]:  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°5ä»¶ã¾ã§
                # æ—¥ä»˜ã‚’å–å¾—
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published = datetime(*entry.updated_parsed[:6])
                
                # å¤ã™ãã‚‹è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
                if published and published < cutoff_date:
                    continue
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã«AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                title = entry.get('title', '')
                summary = entry.get('summary', entry.get('description', ''))[:500]
                
                all_entries.append({
                    "title": title,
                    "summary": summary,
                    "url": entry.get('link', ''),
                    "source": feed_info['name'],
                    "priority": feed_info['priority'],
                    "published": published.isoformat() if published else None,
                    "collected_at": datetime.now().isoformat()
                })
            
            print(f"  âœ… {len(feed.entries[:5])}ä»¶å–å¾—")
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {feed_info['name']} - {str(e)[:50]}")
            continue
    
    # å„ªå…ˆåº¦ã¨æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    all_entries.sort(key=lambda x: (x['priority'], x['published'] or ''), reverse=False)
    
    return all_entries


def filter_ai_news_with_llm(client, entries, max_news=3):
    """LLMã‚’ä½¿ã£ã¦AIé–¢é€£ã®é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸å®šãƒ»è¦ç´„"""
    
    if not entries:
        return []
    
    # ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹
    entries_text = "\n".join([
        f"[{i+1}] {e['source']}: {e['title']}\n    {e['summary'][:200]}...\n    URL: {e['url']}"
        for i, e in enumerate(entries[:20])  # æœ€å¤§20ä»¶ã‹ã‚‰é¸å®š
    ])
    
    prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‹ã‚‰ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«ã¨ã£ã¦æœ€ã‚‚é‡è¦ãªAI/MLé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’{max_news}ä»¶é¸ã‚“ã§ãã ã•ã„ã€‚

ã€é¸å®šåŸºæº–ã€‘
- æ–°ã—ã„AIãƒ¢ãƒ‡ãƒ«ã®ãƒªãƒªãƒ¼ã‚¹ï¼ˆGPT, Claude, Gemini, Llamaç­‰ï¼‰
- é–‹ç™ºè€…å‘ã‘APIãƒ»ãƒ„ãƒ¼ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
- å®Ÿå‹™ã«ç›´çµã™ã‚‹æŠ€è¡“ç™ºè¡¨
- å…·ä½“çš„ãªæ€§èƒ½æ•°å€¤ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚‹ã‚‚ã®

ã€é™¤å¤–åŸºæº–ã€‘
- ä¸€èˆ¬çš„ãªAIè§£èª¬ãƒ»å…¥é–€è¨˜äº‹
- ä¼æ¥­ã®æ¡ç”¨ãƒ»è³‡é‡‘èª¿é”ãƒ‹ãƒ¥ãƒ¼ã‚¹
- è¦åˆ¶ãƒ»æ”¿ç­–é–¢é€£ï¼ˆæŠ€è¡“ç™ºè¡¨ã‚’é™¤ãï¼‰

ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã€‘
{entries_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
é¸ã‚“ã ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç•ªå·ã¨ã€æ—¥æœ¬èªã§ã®è¶…è¦ç´„ï¼ˆ1è¡Œ50æ–‡å­—ä»¥å†…ï¼‰ã‚’JSONé…åˆ—ã§å‡ºåŠ›:
[{{"index": 1, "summary_ja": "GPT-5ãŒç™ºè¡¨ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆé€Ÿåº¦ãŒ3å€ã«å‘ä¸Š"}}, ...]

JSONã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[prompt],
            config=types.GenerateContentConfig(temperature=0.2)
        )
        
        response_text = response.text.strip()
        
        # JSONã‚’æŠ½å‡º
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        json_start = response_text.find('[')
        json_end = response_text.rfind(']') + 1
        if json_start != -1 and json_end > json_start:
            response_text = response_text[json_start:json_end]
        
        selected = json.loads(response_text)
        
        # é¸å®šã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¿”ã™
        result = []
        for item in selected[:max_news]:
            idx = item.get('index', 1) - 1
            if 0 <= idx < len(entries):
                entry = entries[idx]
                entry['summary_ja'] = item.get('summary_ja', entry['title'])
                result.append(entry)
        
        return result
        
    except Exception as e:
        print(f"âš ï¸ LLMé¸å®šã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å„ªå…ˆåº¦é †ã«ä¸Šä½ã‚’è¿”ã™
        return entries[:max_news]


def collect_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    api_key = os.getenv("GEMINI_API_KEY")
    
    if not api_key:
        print("ã‚¨ãƒ©ãƒ¼: GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
    
    client = genai.Client(api_key=api_key)
    print("ğŸ“Š Gemini API ã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸å®šãƒ»è¦ç´„ã—ã¾ã™")
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—
    print("\n" + "=" * 50)
    print("ğŸ“¡ å…¬å¼RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
    print("=" * 50)
    
    entries = fetch_rss_entries(max_age_days=7)
    print(f"\nğŸ“° åˆè¨ˆ {len(entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—")
    
    if not entries:
        print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return []
    
    # LLMã§é¸å®šãƒ»è¦ç´„
    print("\nğŸ¤– LLMã§é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸å®šä¸­...")
    selected_news = filter_ai_news_with_llm(client, entries, max_news=3)
    
    print(f"âœ… {len(selected_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸å®šã—ã¾ã—ãŸ")
    
    # ä¿å­˜å½¢å¼ã«å¤‰æ›
    news_items = []
    for news in selected_news:
        news_items.append({
            "title": news.get('summary_ja', news['title']),
            "summary": news['summary'][:300],
            "url": news['url'],
            "source": news['source'],
            "collected_at": news['collected_at']
        })
    
    # ä¿å­˜
    output_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "news_raw.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(news_items, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ä¿å­˜å…ˆ: {output_path}")
    
    return news_items


if __name__ == "__main__":
    print("=" * 50)
    print("--- ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹• ---")
    print("å…¬å¼RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¾ã™...")
    print("=" * 50)
    
    result = collect_news()
    
    if result:
        print("\n" + "=" * 50)
        print(f"âœ… å®Œäº†ï¼{len(result)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¾ã—ãŸ")
        for i, news in enumerate(result, 1):
            print(f"  {i}. [{news['source']}] {news['title'][:40]}...")
        print("=" * 50)
    else:
        print("\n" + "=" * 50)
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("GEMINI_API_KEYã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("=" * 50)
        sys.exit(1)
